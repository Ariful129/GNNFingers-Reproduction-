{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH5egvdmH9pT",
        "outputId": "32850b9b-c51e-4d80-e7ed-b81ff37138cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 1: Installing Dependencies for Graph Classification\n",
            "======================================================================\n",
            "Installing packages...\n",
            "\n",
            "✓ Directory structure created:\n",
            "  /content/gnnfingers_graph_classification/\n",
            "    ├── data/            (PROTEIN dataset)\n",
            "    ├── models/\n",
            "    │   ├── target/      (target GCN)\n",
            "    │   ├── positive/    (fine-tuned clones)\n",
            "    │   └── negative/    (independent models)\n",
            "    ├── fingerprints/    (synthetic graphs)\n",
            "    ├── verifier/        (binary classifier)\n",
            "    └── results/         (TP/TN/accuracy)\n",
            "\n",
            "✓ Dependencies installed and directories ready\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "GNNFingers - Graph Classification on PROTEIN Dataset\n",
        "=====================================================\n",
        "\n",
        "\n",
        "PIPELINE:\n",
        "1. Load PROTEIN dataset (multiple graphs, each graph = one sample)\n",
        "2. Train target GCN model for graph classification\n",
        "3. Generate 2 positive models (fine-tuned clones)\n",
        "4. Generate 2 negative models (fresh GCN and GraphSAGE)\n",
        "5. Create 5 synthetic fingerprints (random small graphs)\n",
        "6. Collect model responses (graph-level predictions)\n",
        "7. Train verifier (binary classifier)\n",
        "8. Evaluate: TP, TN, Accuracy\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: Setup and Install Dependencies\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 1: Installing Dependencies for Graph Classification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Installing packages...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                       \"torch\", \"torch_geometric\", \"torch_scatter\", \"torch_sparse\"])\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create base directory for graph classification\n",
        "base_dir = Path(\"/content/gnnfingers_graph_classification\")\n",
        "base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "(base_dir / \"data\").mkdir(exist_ok=True)\n",
        "(base_dir / \"models\" / \"target\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"models\" / \"positive\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"models\" / \"negative\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"fingerprints\").mkdir(exist_ok=True)\n",
        "(base_dir / \"verifier\").mkdir(exist_ok=True)\n",
        "(base_dir / \"results\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"\\n✓ Directory structure created:\")\n",
        "print(f\"  {base_dir}/\")\n",
        "print(f\"    ├── data/            (PROTEIN dataset)\")\n",
        "print(f\"    ├── models/\")\n",
        "print(f\"    │   ├── target/      (target GCN)\")\n",
        "print(f\"    │   ├── positive/    (fine-tuned clones)\")\n",
        "print(f\"    │   └── negative/    (independent models)\")\n",
        "print(f\"    ├── fingerprints/    (synthetic graphs)\")\n",
        "print(f\"    ├── verifier/        (binary classifier)\")\n",
        "print(f\"    └── results/         (TP/TN/accuracy)\\n\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✓ Dependencies installed and directories ready\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec1_5sAcPNPO",
        "outputId": "a71fab9b-cf89-47d2-f54c-a1c8c656276b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 2: Define Graph Classification Models\n",
            "======================================================================\n",
            "✓ Graph Classification models defined\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Define Graph Classification Models\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 2: Define Graph Classification Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool, global_add_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "class GCNGraphClassifier(nn.Module):\n",
        "    \"\"\"GCN for Graph Classification with Mean Pooling\"\"\"\n",
        "    def __init__(self, num_features, hidden_channels=64, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Graph-level pooling (mean aggregation)\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "class GraphSAGEClassifier(nn.Module):\n",
        "    \"\"\"GraphSAGE for Graph Classification\"\"\"\n",
        "    def __init__(self, num_features, hidden_channels=64, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.sage1 = SAGEConv(num_features, hidden_channels)\n",
        "        self.sage2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.sage3 = SAGEConv(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.sage1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.sage2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.sage3(x, edge_index)\n",
        "\n",
        "        # Graph pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "class Verifier(nn.Module):\n",
        "    \"\"\"Binary classifier verifier\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x.squeeze()\n",
        "\n",
        "def load_protein_dataset():\n",
        "    \"\"\"Load PROTEIN dataset for graph classification\"\"\"\n",
        "    dataset = TUDataset(root=str(base_dir / \"data\"), name='PROTEINS')\n",
        "\n",
        "    # Split dataset: 80% train, 10% val, 10% test\n",
        "    torch.manual_seed(42)\n",
        "    dataset = dataset.shuffle()\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = int(0.1 * len(dataset))\n",
        "\n",
        "    train_dataset = dataset[:train_size]\n",
        "    val_dataset = dataset[train_size:train_size + val_size]\n",
        "    test_dataset = dataset[train_size + val_size:]\n",
        "\n",
        "    return dataset, train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def train_graph_classifier(model, train_loader, val_loader, epochs=50, lr=0.001, verbose=True):\n",
        "    \"\"\"Train a graph classification model\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.cross_entropy(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for data in val_loader:\n",
        "                    out = model(data.x, data.edge_index, data.batch)\n",
        "                    pred = out.argmax(dim=1)\n",
        "                    correct += (pred == data.y).sum().item()\n",
        "                    total += data.y.size(0)\n",
        "\n",
        "            val_acc = correct / total\n",
        "            print(f\"    Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"✓ Graph Classification models defined\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVn6EMUAPXtD",
        "outputId": "8e5322c5-0f7a-460d-bff6-b913d6a73778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 3: Load PROTEIN Dataset and Train Target Model\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: PROTEINS (Graph Classification)\n",
            "  Total Graphs: 1113\n",
            "  Num Classes: 2\n",
            "  Num Features: 3\n",
            "  Train/Val/Test: 890/111/112\n",
            "\n",
            "Training TARGET model (GCN for Graph Classification)...\n",
            "    Epoch 10/50 | Loss: 0.6248 | Val Acc: 0.676\n",
            "    Epoch 20/50 | Loss: 0.6122 | Val Acc: 0.649\n",
            "    Epoch 30/50 | Loss: 0.6112 | Val Acc: 0.694\n",
            "    Epoch 40/50 | Loss: 0.6128 | Val Acc: 0.703\n",
            "    Epoch 50/50 | Loss: 0.6056 | Val Acc: 0.649\n",
            "\n",
            "✓ Target model saved to /content/gnnfingers_graph_classification/models/target/gcn_graph_target.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Load PROTEIN Dataset and Train Target Model\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 3: Load PROTEIN Dataset and Train Target Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dataset, train_dataset, val_dataset, test_dataset = load_protein_dataset()\n",
        "\n",
        "print(f\"\\nDataset: PROTEINS (Graph Classification)\")\n",
        "print(f\"  Total Graphs: {len(dataset)}\")\n",
        "print(f\"  Num Classes: {dataset.num_classes}\")\n",
        "print(f\"  Num Features: {dataset.num_features}\")\n",
        "print(f\"  Train/Val/Test: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Train target model\n",
        "print(f\"\\nTraining TARGET model (GCN for Graph Classification)...\")\n",
        "target_model = GCNGraphClassifier(\n",
        "    num_features=dataset.num_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset.num_classes\n",
        ")\n",
        "target_model = train_graph_classifier(target_model, train_loader, val_loader, epochs=50)\n",
        "\n",
        "# Save target model\n",
        "target_path = base_dir / \"models\" / \"target\" / \"gcn_graph_target.pt\"\n",
        "torch.save(target_model.state_dict(), target_path)\n",
        "print(f\"\\n✓ Target model saved to {target_path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb0L8wrYPgGj",
        "outputId": "b4b55176-19c7-4fa3-81ea-b1c66fe4ec2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 4: Generate Positive Models (Fine-tuned Clones)\n",
            "======================================================================\n",
            "\n",
            "Creating POSITIVE model 1 (fine-tuned clone)...\n",
            "  ✓ Saved to /content/gnnfingers_graph_classification/models/positive/gcn_graph_pos_0.pt\n",
            "\n",
            "Creating POSITIVE model 2 (fine-tuned clone)...\n",
            "  ✓ Saved to /content/gnnfingers_graph_classification/models/positive/gcn_graph_pos_1.pt\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Generate 2 Positive Models (Fine-tuned)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 4: Generate Positive Models (Fine-tuned Clones)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def clone_and_finetune_graph(model, train_loader, seed, finetune_epochs=10, lr=0.0001):\n",
        "    \"\"\"Clone target graph classifier and fine-tune\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    cloned = GCNGraphClassifier(\n",
        "        num_features=dataset.num_features,\n",
        "        hidden_channels=64,\n",
        "        num_classes=dataset.num_classes\n",
        "    )\n",
        "    cloned.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(cloned.parameters(), lr=lr)\n",
        "\n",
        "    for _ in range(finetune_epochs):\n",
        "        cloned.train()\n",
        "        for data in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = cloned(data.x, data.edge_index, data.batch)\n",
        "            loss = F.cross_entropy(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return cloned\n",
        "\n",
        "positive_models = []\n",
        "positive_paths = []\n",
        "\n",
        "for i in range(2):\n",
        "    print(f\"\\nCreating POSITIVE model {i+1} (fine-tuned clone)...\")\n",
        "    pos_model = clone_and_finetune_graph(target_model, train_loader, seed=100+i, finetune_epochs=10)\n",
        "    positive_models.append(pos_model)\n",
        "\n",
        "    # Save model\n",
        "    pos_path = base_dir / \"models\" / \"positive\" / f\"gcn_graph_pos_{i}.pt\"\n",
        "    torch.save(pos_model.state_dict(), pos_path)\n",
        "    positive_paths.append(pos_path)\n",
        "    print(f\"  ✓ Saved to {pos_path}\")\n",
        "\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv98XLf5PmPs",
        "outputId": "39558a02-2809-4263-8281-b7c81a187f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 5: Generate Negative Models (Independent Training)\n",
            "======================================================================\n",
            "\n",
            "Creating NEGATIVE model 1 (fresh GCN, different seed)...\n",
            "  ✓ Saved to /content/gnnfingers_graph_classification/models/negative/gcn_graph_neg_0.pt\n",
            "\n",
            "Creating NEGATIVE model 2 (GraphSAGE, different architecture)...\n",
            "  ✓ Saved to /content/gnnfingers_graph_classification/models/negative/sage_graph_neg_1.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Generate 2 Negative Models (Independent)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 5: Generate Negative Models (Independent Training)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "negative_models = []\n",
        "negative_paths = []\n",
        "\n",
        "# Negative 1: Fresh GCN\n",
        "print(\"\\nCreating NEGATIVE model 1 (fresh GCN, different seed)...\")\n",
        "torch.manual_seed(200)\n",
        "neg_model_1 = GCNGraphClassifier(\n",
        "    num_features=dataset.num_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset.num_classes\n",
        ")\n",
        "neg_model_1 = train_graph_classifier(neg_model_1, train_loader, val_loader, epochs=50, verbose=False)\n",
        "negative_models.append(neg_model_1)\n",
        "\n",
        "neg_path_1 = base_dir / \"models\" / \"negative\" / \"gcn_graph_neg_0.pt\"\n",
        "torch.save(neg_model_1.state_dict(), neg_path_1)\n",
        "negative_paths.append(neg_path_1)\n",
        "print(f\"  ✓ Saved to {neg_path_1}\\n\")\n",
        "\n",
        "# Negative 2: GraphSAGE\n",
        "print(\"Creating NEGATIVE model 2 (GraphSAGE, different architecture)...\")\n",
        "torch.manual_seed(201)\n",
        "neg_model_2 = GraphSAGEClassifier(\n",
        "    num_features=dataset.num_features,\n",
        "    hidden_channels=64,\n",
        "    num_classes=dataset.num_classes\n",
        ")\n",
        "neg_model_2 = train_graph_classifier(neg_model_2, train_loader, val_loader, epochs=50, verbose=False)\n",
        "negative_models.append(neg_model_2)\n",
        "\n",
        "neg_path_2 = base_dir / \"models\" / \"negative\" / \"sage_graph_neg_1.pt\"\n",
        "torch.save(neg_model_2.state_dict(), neg_path_2)\n",
        "negative_paths.append(neg_path_2)\n",
        "print(f\"  ✓ Saved to {neg_path_2}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIie6GS0PzD2",
        "outputId": "885964f3-91b5-45d1-fc74-2f6a629c388e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 6: Create Synthetic Fingerprints for Graph Classification\n",
            "======================================================================\n",
            "\n",
            "Creating 5 random graph fingerprints...\n",
            "  ✓ FP 1: nodes=20, edges=39\n",
            "  ✓ FP 2: nodes=20, edges=40\n",
            "  ✓ FP 3: nodes=20, edges=40\n",
            "  ✓ FP 4: nodes=20, edges=36\n",
            "  ✓ FP 5: nodes=20, edges=38\n",
            "\n",
            "✓ Fingerprints saved to /content/gnnfingers_graph_classification/fingerprints/graph_fingerprints.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Create Synthetic Fingerprints (Random Graphs)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 6: Create Synthetic Fingerprints for Graph Classification\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "num_fingerprints = 5\n",
        "nodes_per_fp = 20\n",
        "\n",
        "def create_random_graph_fingerprint(num_nodes, num_features, sparsity=0.3):\n",
        "    \"\"\"\n",
        "    Create random synthetic graph fingerprint.\n",
        "    For graph classification, each fingerprint is a complete graph object.\n",
        "    \"\"\"\n",
        "    # Random node features\n",
        "    x = torch.randn(num_nodes, num_features)\n",
        "\n",
        "    # Random sparse edges\n",
        "    num_possible_edges = num_nodes * (num_nodes - 1) // 2\n",
        "    num_edges = max(1, int(num_possible_edges * sparsity))\n",
        "\n",
        "    edge_pairs = []\n",
        "    for _ in range(num_edges):\n",
        "        u = np.random.randint(0, num_nodes)\n",
        "        v = np.random.randint(0, num_nodes)\n",
        "        if u != v and [u, v] not in edge_pairs and [v, u] not in edge_pairs:\n",
        "            edge_pairs.append([u, v])\n",
        "            edge_pairs.append([v, u])  # Make undirected\n",
        "\n",
        "    if edge_pairs:\n",
        "        edge_index = torch.tensor(edge_pairs, dtype=torch.long).t().contiguous()\n",
        "    else:\n",
        "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "\n",
        "    # Batch tensor (all nodes belong to graph 0)\n",
        "    batch = torch.zeros(num_nodes, dtype=torch.long)\n",
        "\n",
        "    return x, edge_index, batch\n",
        "\n",
        "fingerprints = []\n",
        "print(f\"\\nCreating {num_fingerprints} random graph fingerprints...\")\n",
        "\n",
        "for i in range(num_fingerprints):\n",
        "    x, edge_index, batch = create_random_graph_fingerprint(\n",
        "        nodes_per_fp,\n",
        "        dataset.num_features,\n",
        "        sparsity=0.25\n",
        "    )\n",
        "    fingerprints.append((x, edge_index, batch))\n",
        "    print(f\"  ✓ FP {i+1}: nodes={x.shape[0]}, edges={edge_index.shape[1]//2}\")\n",
        "\n",
        "# Save fingerprints\n",
        "fp_path = base_dir / \"fingerprints\" / \"graph_fingerprints.pt\"\n",
        "torch.save(fingerprints, fp_path)\n",
        "print(f\"\\n✓ Fingerprints saved to {fp_path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBVeR347P0ls",
        "outputId": "c9bbf760-017a-4f30-9585-cd38684b1914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 7: Collect Model Response Vectors (Graph Predictions)\n",
            "======================================================================\n",
            "\n",
            "Collecting graph prediction responses from TARGET model...\n",
            "Collecting responses from POSITIVE model 0...\n",
            "Collecting responses from POSITIVE model 1...\n",
            "Collecting responses from NEGATIVE model 0...\n",
            "Collecting responses from NEGATIVE model 1...\n",
            "\n",
            "✓ Response vector dimension: 10\n",
            "  (= 5 fingerprints × 2 classes)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CELL 7: Collect Model Response Vectors (Graph Predictions)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 7: Collect Model Response Vectors (Graph Predictions)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def get_graph_response_vector(model, fingerprints):\n",
        "    \"\"\"\n",
        "    Query model on fingerprints and collect graph-level predictions.\n",
        "    For graph classification, each fingerprint produces one prediction vector.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    responses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for fp_x, fp_edge, fp_batch in fingerprints:\n",
        "            # Get graph-level prediction\n",
        "            out = model(fp_x, fp_edge, fp_batch)  # [1, num_classes]\n",
        "            responses.append(out.flatten())\n",
        "\n",
        "    # Concatenate all responses into one vector\n",
        "    response_vector = torch.cat(responses)\n",
        "    return response_vector\n",
        "\n",
        "# Collect responses from all models\n",
        "all_responses = {}\n",
        "\n",
        "print(\"\\nCollecting graph prediction responses from TARGET model...\")\n",
        "all_responses['target'] = get_graph_response_vector(target_model, fingerprints)\n",
        "\n",
        "for i, pos_model in enumerate(positive_models):\n",
        "    print(f\"Collecting responses from POSITIVE model {i}...\")\n",
        "    all_responses[f'pos_{i}'] = get_graph_response_vector(pos_model, fingerprints)\n",
        "\n",
        "for i, neg_model in enumerate(negative_models):\n",
        "    print(f\"Collecting responses from NEGATIVE model {i}...\")\n",
        "    all_responses[f'neg_{i}'] = get_graph_response_vector(neg_model, fingerprints)\n",
        "\n",
        "print(f\"\\n✓ Response vector dimension: {all_responses['target'].shape[0]}\")\n",
        "print(f\"  (= {num_fingerprints} fingerprints × {dataset.num_classes} classes)\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6bRk4Y0P4S0",
        "outputId": "e27e3fed-669e-43c4-b88a-074569a2424d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 8: Build Training Data and Train Verifier\n",
            "======================================================================\n",
            "\n",
            "✓ Target model (label=1)\n",
            "✓ Positive model 0 (label=1)\n",
            "✓ Positive model 1 (label=1)\n",
            "✓ Negative model 0 (label=0)\n",
            "✓ Negative model 1 (label=0)\n",
            "\n",
            "Training data shape: X=torch.Size([5, 10]), y=torch.Size([5])\n",
            "  Class 1 (positive): 3 samples\n",
            "  Class 0 (negative): 2 samples\n",
            "\n",
            "Training VERIFIER for Graph Classification task...\n",
            "  Epoch 50/200 | Loss: 0.0120\n",
            "  Epoch 100/200 | Loss: 0.0004\n",
            "  Epoch 150/200 | Loss: 0.0002\n",
            "  Epoch 200/200 | Loss: 0.0002\n",
            "\n",
            "✓ Verifier saved to /content/gnnfingers_graph_classification/verifier/verifier_graph.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Build Training Data and Train Verifier\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 8: Build Training Data and Train Verifier\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Build training dataset for verifier\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "# Positive samples (label = 1) - target + fine-tuned models\n",
        "X_train.append(all_responses['target'].unsqueeze(0))\n",
        "y_train.append(1)\n",
        "print(\"\\n✓ Target model (label=1)\")\n",
        "\n",
        "for i in range(len(positive_models)):\n",
        "    X_train.append(all_responses[f'pos_{i}'].unsqueeze(0))\n",
        "    y_train.append(1)\n",
        "    print(f\"✓ Positive model {i} (label=1)\")\n",
        "\n",
        "# Negative samples (label = 0) - independent models\n",
        "for i in range(len(negative_models)):\n",
        "    X_train.append(all_responses[f'neg_{i}'].unsqueeze(0))\n",
        "    y_train.append(0)\n",
        "    print(f\"✓ Negative model {i} (label=0)\")\n",
        "\n",
        "X_train = torch.cat(X_train, dim=0)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "print(f\"\\nTraining data shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Class 1 (positive): {(y_train == 1).sum()} samples\")\n",
        "print(f\"  Class 0 (negative): {(y_train == 0).sum()} samples\")\n",
        "\n",
        "# Train verifier\n",
        "print(f\"\\nTraining VERIFIER for Graph Classification task...\")\n",
        "verifier = Verifier(input_dim=X_train.shape[1], hidden_dim=32)\n",
        "optimizer = torch.optim.Adam(verifier.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    verifier.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = verifier(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save verifier\n",
        "verifier_path = base_dir / \"verifier\" / \"verifier_graph.pt\"\n",
        "torch.save(verifier.state_dict(), verifier_path)\n",
        "print(f\"\\n✓ Verifier saved to {verifier_path}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2uB1fdTP7u7",
        "outputId": "8b3cba19-048f-42e0-aac9-6405508babcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 9: EVALUATE VERIFIER - Calculate TP/TN/Accuracy\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CONFUSION MATRIX\n",
            "======================================================================\n",
            "  TP (True Positive):   3   ← Positive models correctly identified\n",
            "  TN (True Negative):   2   ← Negative models correctly identified\n",
            "  FP (False Positive):  0   ← Negative incorrectly as positive\n",
            "  FN (False Negative):  0   ← Positive incorrectly as negative\n",
            "\n",
            "======================================================================\n",
            "METRICS\n",
            "======================================================================\n",
            "  Accuracy:   1.000  (TP+TN)/Total = (3+2)/5\n",
            "  Precision:  1.000  TP/(TP+FP) = 3/(3+0)\n",
            "  Recall:     1.000   TP/(TP+FN) = 3/(3+0)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Evaluate Verifier and Calculate Metrics\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 9: EVALUATE VERIFIER - Calculate TP/TN/Accuracy\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "verifier.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = verifier(X_train)\n",
        "    y_pred = (y_pred_probs >= 0.5).long()\n",
        "    y_true = y_train.long()\n",
        "\n",
        "# Calculate confusion matrix\n",
        "TP = ((y_pred == 1) & (y_true == 1)).sum().item()\n",
        "TN = ((y_pred == 0) & (y_true == 0)).sum().item()\n",
        "FP = ((y_pred == 1) & (y_true == 0)).sum().item()\n",
        "FN = ((y_pred == 0) & (y_true == 1)).sum().item()\n",
        "\n",
        "total = len(y_true)\n",
        "accuracy = (TP + TN) / total\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  TP (True Positive):   {TP}   ← Positive models correctly identified\")\n",
        "print(f\"  TN (True Negative):   {TN}   ← Negative models correctly identified\")\n",
        "print(f\"  FP (False Positive):  {FP}   ← Negative incorrectly as positive\")\n",
        "print(f\"  FN (False Negative):  {FN}   ← Positive incorrectly as negative\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"METRICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Accuracy:   {accuracy:.3f}  (TP+TN)/Total = ({TP}+{TN})/{total}\")\n",
        "print(f\"  Precision:  {precision:.3f}  TP/(TP+FP) = {TP}/({TP}+{FP})\")\n",
        "print(f\"  Recall:     {recall:.3f}   TP/(TP+FN) = {TP}/({TP}+{FN})\")\n",
        "print()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
