{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT4pgdj3QTat",
        "outputId": "f6cbcfc5-409f-4e8a-a35c-505497a511d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 1: Installing Dependencies for Graph Matching\n",
            "======================================================================\n",
            "Installing packages...\n",
            "\n",
            "✓ Directory structure created:\n",
            "  /content/gnnfingers_graph_matching/\n",
            "    ├── data/            (AIDS dataset)\n",
            "    ├── models/\n",
            "    │   ├── target/      (target GCN)\n",
            "    │   ├── positive/    (fine-tuned clones)\n",
            "    │   └── negative/    (independent models)\n",
            "    ├── fingerprints/    (synthetic graph pairs)\n",
            "    ├── verifier/        (binary classifier)\n",
            "    └── results/         (TP/TN/accuracy)\n",
            "\n",
            "✓ Dependencies installed and directories ready\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "GNNFingers - Graph Matching on AIDS Dataset\n",
        "============================================\n",
        "\n",
        "PIPELINE:\n",
        "1. Load AIDS dataset (pairs of graphs for similarity computation)\n",
        "2. Train target GCN model for graph matching\n",
        "3. Generate 2 positive models (fine-tuned clones)\n",
        "4. Generate 2 negative models (fresh GCN and SimGNN)\n",
        "5. Create 5 synthetic fingerprints (pairs of random graphs)\n",
        "6. Collect model responses (similarity scores between graph pairs)\n",
        "7. Train verifier (binary classifier)\n",
        "8. Evaluate: TP, TN, Accuracy\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: Setup and Install Dependencies\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 1: Installing Dependencies for Graph Matching\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Installing packages...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                       \"torch\", \"torch_geometric\", \"torch_scatter\", \"torch_sparse\"])\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create base directory for graph matching\n",
        "base_dir = Path(\"/content/gnnfingers_graph_matching\")\n",
        "base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "(base_dir / \"data\").mkdir(exist_ok=True)\n",
        "(base_dir / \"models\" / \"target\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"models\" / \"positive\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"models\" / \"negative\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"fingerprints\").mkdir(exist_ok=True)\n",
        "(base_dir / \"verifier\").mkdir(exist_ok=True)\n",
        "(base_dir / \"results\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"\\n✓ Directory structure created:\")\n",
        "print(f\"  {base_dir}/\")\n",
        "print(f\"    ├── data/            (AIDS dataset)\")\n",
        "print(f\"    ├── models/\")\n",
        "print(f\"    │   ├── target/      (target GCN)\")\n",
        "print(f\"    │   ├── positive/    (fine-tuned clones)\")\n",
        "print(f\"    │   └── negative/    (independent models)\")\n",
        "print(f\"    ├── fingerprints/    (synthetic graph pairs)\")\n",
        "print(f\"    ├── verifier/        (binary classifier)\")\n",
        "print(f\"    └── results/         (TP/TN/accuracy)\\n\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✓ Dependencies installed and directories ready\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cgNc1IlsUt",
        "outputId": "09365144-45ec-4cee-9bc6-2c194a14c776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 2: Define Graph Matching Models\n",
            "======================================================================\n",
            "✓ Graph Matching models defined\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Define Graph Matching Models\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 2: Define Graph Matching Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool, global_add_pool\n",
        "from torch_geometric.datasets import GEDDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "class GraphMatcherGCN(nn.Module):\n",
        "    \"\"\"GCN for Graph Matching (computes similarity between two graphs)\"\"\"\n",
        "    def __init__(self, num_features, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # NTN (Neural Tensor Network) layer for similarity\n",
        "        self.ntn_weight = nn.Parameter(torch.randn(hidden_channels, hidden_channels, 16))\n",
        "        self.ntn_bias = nn.Parameter(torch.randn(16))\n",
        "        self.linear = nn.Linear(16, 1)\n",
        "\n",
        "    def encode(self, x, edge_index, batch):\n",
        "        \"\"\"Encode graph to embedding\"\"\"\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Graph-level pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        \"\"\"\n",
        "        Compute similarity between two graphs.\n",
        "        data1, data2: (x, edge_index, batch) tuples\n",
        "        \"\"\"\n",
        "        x1, edge_index1, batch1 = data1\n",
        "        x2, edge_index2, batch2 = data2\n",
        "\n",
        "        # Get graph embeddings\n",
        "        h1 = self.encode(x1, edge_index1, batch1)  # [batch_size, hidden]\n",
        "        h2 = self.encode(x2, edge_index2, batch2)\n",
        "\n",
        "        # Compute similarity via Neural Tensor Network\n",
        "        # scores[i] = h1^T W[i] h2\n",
        "        scores = []\n",
        "        for i in range(16):\n",
        "            score = torch.sum(h1 * torch.mm(h2, self.ntn_weight[:, :, i].t()), dim=1)\n",
        "            scores.append(score)\n",
        "        scores = torch.stack(scores, dim=1) + self.ntn_bias\n",
        "\n",
        "        # Final similarity score\n",
        "        similarity = self.linear(F.relu(scores))\n",
        "        return similarity.squeeze()\n",
        "\n",
        "class GraphMatcherSimGNN(nn.Module):\n",
        "    \"\"\"Simplified SimGNN-style architecture\"\"\"\n",
        "    def __init__(self, num_features, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_channels, 1)\n",
        "\n",
        "        # Similarity predictor\n",
        "        self.fc1 = nn.Linear(hidden_channels * 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def encode(self, x, edge_index, batch):\n",
        "        \"\"\"Encode with attention pooling\"\"\"\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # Attention weights\n",
        "        att_weights = torch.sigmoid(self.attention(x))\n",
        "        x = x * att_weights\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        \"\"\"Compute graph similarity\"\"\"\n",
        "        x1, edge_index1, batch1 = data1\n",
        "        x2, edge_index2, batch2 = data2\n",
        "\n",
        "        h1 = self.encode(x1, edge_index1, batch1)\n",
        "        h2 = self.encode(x2, edge_index2, batch2)\n",
        "\n",
        "        # Concatenate and predict similarity\n",
        "        h = torch.cat([h1, h2], dim=1)\n",
        "        h = F.relu(self.fc1(h))\n",
        "        similarity = self.fc2(h)\n",
        "\n",
        "        return similarity.squeeze()\n",
        "\n",
        "class Verifier(nn.Module):\n",
        "    \"\"\"Binary classifier verifier\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x.squeeze()\n",
        "\n",
        "def load_aids_dataset():\n",
        "    \"\"\"Load AIDS dataset for graph matching\"\"\"\n",
        "    # Download AIDS dataset\n",
        "    dataset = GEDDataset(root=str(base_dir / \"data\"), name='AIDS700nef')\n",
        "\n",
        "    # Create graph pairs for training\n",
        "    # For simplicity, we'll create random pairs\n",
        "    torch.manual_seed(42)\n",
        "    num_pairs = 500\n",
        "    pairs = []\n",
        "\n",
        "    for _ in range(num_pairs):\n",
        "        idx1 = np.random.randint(0, len(dataset))\n",
        "        idx2 = np.random.randint(0, len(dataset))\n",
        "\n",
        "        # Normalized GED as ground truth similarity (0=identical, 1=very different)\n",
        "        # We'll simulate this with random values for toy example\n",
        "        ged = np.random.uniform(0, 10)\n",
        "        normalized_sim = 1.0 / (1.0 + ged)  # Convert to similarity\n",
        "\n",
        "        pairs.append((idx1, idx2, normalized_sim))\n",
        "\n",
        "    return dataset, pairs\n",
        "\n",
        "def train_graph_matcher(model, dataset, pairs, epochs=50, lr=0.001, verbose=True):\n",
        "    \"\"\"Train a graph matching model\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Split pairs into train/val\n",
        "    train_pairs = pairs[:int(0.8 * len(pairs))]\n",
        "    val_pairs = pairs[int(0.8 * len(pairs)):]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Sample mini-batches\n",
        "        np.random.shuffle(train_pairs)\n",
        "        batch_size = 32\n",
        "\n",
        "        for i in range(0, len(train_pairs), batch_size):\n",
        "            batch_pairs = train_pairs[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "\n",
        "            for idx1, idx2, true_sim in batch_pairs:\n",
        "                g1 = dataset[idx1]\n",
        "                g2 = dataset[idx2]\n",
        "\n",
        "                # Create batch tensors\n",
        "                batch1 = torch.zeros(g1.x.shape[0], dtype=torch.long)\n",
        "                batch2 = torch.zeros(g2.x.shape[0], dtype=torch.long)\n",
        "\n",
        "                # Predict similarity\n",
        "                pred_sim = model((g1.x, g1.edge_index, batch1),\n",
        "                                (g2.x, g2.edge_index, batch2))\n",
        "\n",
        "                # MSE loss\n",
        "                loss = F.mse_loss(pred_sim.unsqueeze(0), torch.tensor([true_sim]))\n",
        "                batch_loss += loss\n",
        "\n",
        "            batch_loss = batch_loss / len(batch_pairs)\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += batch_loss.item()\n",
        "\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for idx1, idx2, true_sim in val_pairs[:50]:  # Sample for speed\n",
        "                    g1 = dataset[idx1]\n",
        "                    g2 = dataset[idx2]\n",
        "                    batch1 = torch.zeros(g1.x.shape[0], dtype=torch.long)\n",
        "                    batch2 = torch.zeros(g2.x.shape[0], dtype=torch.long)\n",
        "\n",
        "                    pred_sim = model((g1.x, g1.edge_index, batch1),\n",
        "                                    (g2.x, g2.edge_index, batch2))\n",
        "                    val_loss += F.mse_loss(pred_sim.unsqueeze(0), torch.tensor([true_sim])).item()\n",
        "\n",
        "            val_loss = val_loss / min(50, len(val_pairs))\n",
        "            print(f\"    Epoch {epoch+1}/{epochs} | Train Loss: {total_loss/(len(train_pairs)//batch_size):.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"✓ Graph Matching models defined\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6FpXq6impRi",
        "outputId": "6d69f131-342d-47ae-eaaa-3e462a0f6a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 3: Load AIDS Dataset and Train Target Model\n",
            "======================================================================\n",
            "\n",
            "Dataset: AIDS (Graph Matching)\n",
            "  Total Graphs: 2000\n",
            "  Num Features: 38\n",
            "  Graph Pairs for Training: 1999\n",
            "\n",
            "Training TARGET model (GCN for Graph Matching)...\n",
            "    Epoch 10/50 | Train Loss: 0.0001 | Val Loss: 0.0002\n",
            "    Epoch 20/50 | Train Loss: 0.0000 | Val Loss: 0.0002\n",
            "    Epoch 30/50 | Train Loss: 0.0000 | Val Loss: 0.0002\n",
            "    Epoch 40/50 | Train Loss: 0.0000 | Val Loss: 0.0002\n",
            "    Epoch 50/50 | Train Loss: 0.0000 | Val Loss: 0.0002\n",
            "\n",
            "✓ Target model saved to models/target/gcn_match_target.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Load AIDS Dataset and Train Target Model\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 3: Load AIDS Dataset and Train Target Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Load AIDS Dataset (using TUDataset)\n",
        "# ----------------------------------------------------------------------------\n",
        "def load_aids_dataset():\n",
        "    dataset = TUDataset(root=\"data/AIDS\", name=\"AIDS\")\n",
        "\n",
        "    # Each pair now includes a label (true_sim)\n",
        "    # Here we assign dummy similarity values:\n",
        "    # - 1.0 for similar graphs (example placeholder)\n",
        "    # You can change this logic if your task defines similarity differently.\n",
        "    pairs = [(i, (i + 1) % len(dataset), 1.0) for i in range(len(dataset) - 1)]\n",
        "\n",
        "    return dataset, pairs\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset, pairs = load_aids_dataset()\n",
        "\n",
        "print(f\"\\nDataset: AIDS (Graph Matching)\")\n",
        "print(f\"  Total Graphs: {len(dataset)}\")\n",
        "print(f\"  Num Features: {dataset.num_node_features}\")\n",
        "print(f\"  Graph Pairs for Training: {len(pairs)}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Train target model (GCN for Graph Matching)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(f\"\\nTraining TARGET model (GCN for Graph Matching)...\")\n",
        "\n",
        "target_model = GraphMatcherGCN(\n",
        "    num_features=dataset.num_node_features,\n",
        "    hidden_channels=64\n",
        ")\n",
        "\n",
        "target_model = train_graph_matcher(target_model, dataset, pairs, epochs=50)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Save target model\n",
        "# ----------------------------------------------------------------------------\n",
        "base_dir = Path(\".\")\n",
        "save_dir = base_dir / \"models\" / \"target\"\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "target_path = save_dir / \"gcn_match_target.pt\"\n",
        "torch.save(target_model.state_dict(), target_path)\n",
        "\n",
        "print(f\"\\n✓ Target model saved to {target_path}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHzGnK1JpSae",
        "outputId": "55f0bde8-3386-49df-acb7-2640958e754c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 4: Generate Positive Models (Fine-tuned Clones)\n",
            "======================================================================\n",
            "\n",
            "Creating POSITIVE model 1 (fine-tuned clone)...\n",
            "  ✓ Saved to models/positive/gcn_match_pos_0.pt\n",
            "\n",
            "Creating POSITIVE model 2 (fine-tuned clone)...\n",
            "  ✓ Saved to models/positive/gcn_match_pos_1.pt\n",
            "\n",
            "✓ All positive models created successfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Generate 2 Positive Models (Fine-tuned)\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 4: Generate Positive Models (Fine-tuned Clones)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Clone and Fine-tune Function\n",
        "# ----------------------------------------------------------------------------\n",
        "def clone_and_finetune_matcher(model, dataset, pairs, seed, finetune_epochs=10, lr=0.0001):\n",
        "    \"\"\"Clone target graph matcher and fine-tune\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    cloned = GraphMatcherGCN(\n",
        "        num_features=dataset.num_node_features,  # FIXED\n",
        "        hidden_channels=64\n",
        "    )\n",
        "    cloned.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(cloned.parameters(), lr=lr)\n",
        "    train_pairs = pairs[:int(0.8 * len(pairs))]\n",
        "\n",
        "    for _ in range(finetune_epochs):\n",
        "        cloned.train()\n",
        "        np.random.shuffle(train_pairs)\n",
        "\n",
        "        for i in range(0, min(100, len(train_pairs)), 10):\n",
        "            batch_pairs = train_pairs[i:i+10]\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "\n",
        "            for idx1, idx2, true_sim in batch_pairs:\n",
        "                g1 = dataset[idx1]\n",
        "                g2 = dataset[idx2]\n",
        "                batch1 = torch.zeros(g1.x.shape[0], dtype=torch.long)\n",
        "                batch2 = torch.zeros(g2.x.shape[0], dtype=torch.long)\n",
        "\n",
        "                pred_sim = cloned((g1.x, g1.edge_index, batch1),\n",
        "                                  (g2.x, g2.edge_index, batch2))\n",
        "\n",
        "                loss = F.mse_loss(pred_sim.unsqueeze(0), torch.tensor([true_sim], dtype=torch.float))\n",
        "                batch_loss += loss\n",
        "\n",
        "            batch_loss = batch_loss / len(batch_pairs)\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return cloned\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Generate Fine-tuned Positive Models\n",
        "# ----------------------------------------------------------------------------\n",
        "positive_models = []\n",
        "positive_paths = []\n",
        "\n",
        "# Create the save directory first (FIXED)\n",
        "pos_dir = Path(\"models/positive\")\n",
        "pos_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for i in range(2):\n",
        "    print(f\"\\nCreating POSITIVE model {i+1} (fine-tuned clone)...\")\n",
        "    pos_model = clone_and_finetune_matcher(target_model, dataset, pairs, seed=100+i, finetune_epochs=10)\n",
        "    positive_models.append(pos_model)\n",
        "\n",
        "    # Save model\n",
        "    pos_path = pos_dir / f\"gcn_match_pos_{i}.pt\"\n",
        "    torch.save(pos_model.state_dict(), pos_path)\n",
        "    positive_paths.append(pos_path)\n",
        "    print(f\"  ✓ Saved to {pos_path}\")\n",
        "\n",
        "print(\"\\n✓ All positive models created successfully!\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL2NpU_Rpyq8",
        "outputId": "bf92b4f2-8634-4fa5-fec5-f832f9642082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 5: Generate Negative Models (Independent Training)\n",
            "======================================================================\n",
            "\n",
            "Creating NEGATIVE model 1 (fresh GCN, different seed)...\n",
            "  ✓ Saved to models/negative/gcn_match_neg_0.pt\n",
            "\n",
            "Creating NEGATIVE model 2 (SimGNN-style, different architecture)...\n",
            "  ✓ Saved to models/negative/simgnn_match_neg_1.pt\n",
            "\n",
            "\n",
            "✓ All negative models created successfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Generate 2 Negative Models (Independent)\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 5: Generate Negative Models (Independent Training)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "negative_models = []\n",
        "negative_paths = []\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Ensure save directory exists (FIXED)\n",
        "# ----------------------------------------------------------------------------\n",
        "neg_dir = Path(\"models/negative\")\n",
        "neg_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Negative 1: Fresh GCN (trained from scratch)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\nCreating NEGATIVE model 1 (fresh GCN, different seed)...\")\n",
        "torch.manual_seed(200)\n",
        "\n",
        "neg_model_1 = GraphMatcherGCN(\n",
        "    num_features=dataset.num_node_features,  # FIXED for PyG\n",
        "    hidden_channels=64\n",
        ")\n",
        "neg_model_1 = train_graph_matcher(neg_model_1, dataset, pairs, epochs=50, verbose=False)\n",
        "negative_models.append(neg_model_1)\n",
        "\n",
        "neg_path_1 = neg_dir / \"gcn_match_neg_0.pt\"\n",
        "torch.save(neg_model_1.state_dict(), neg_path_1)\n",
        "negative_paths.append(neg_path_1)\n",
        "print(f\"  ✓ Saved to {neg_path_1}\\n\")\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Negative 2: SimGNN-style model (different architecture)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"Creating NEGATIVE model 2 (SimGNN-style, different architecture)...\")\n",
        "torch.manual_seed(201)\n",
        "\n",
        "neg_model_2 = GraphMatcherSimGNN(\n",
        "    num_features=dataset.num_node_features,  # FIXED for PyG\n",
        "    hidden_channels=64\n",
        ")\n",
        "neg_model_2 = train_graph_matcher(neg_model_2, dataset, pairs, epochs=50, verbose=False)\n",
        "negative_models.append(neg_model_2)\n",
        "\n",
        "neg_path_2 = neg_dir / \"simgnn_match_neg_1.pt\"\n",
        "torch.save(neg_model_2.state_dict(), neg_path_2)\n",
        "negative_paths.append(neg_path_2)\n",
        "print(f\"  ✓ Saved to {neg_path_2}\\n\")\n",
        "\n",
        "print(\"\\n✓ All negative models created successfully!\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCPY9d2kwBt5",
        "outputId": "47f24e68-bcbf-4977-ccf2-2ea702dfd119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 6: Create Synthetic Fingerprints for Graph Matching\n",
            "======================================================================\n",
            "\n",
            "Creating 5 random graph pair fingerprints...\n",
            "  ✓ FP 1: G1(nodes=15, edges=24) <-> G2(nodes=15, edges=24)\n",
            "  ✓ FP 2: G1(nodes=15, edges=25) <-> G2(nodes=15, edges=24)\n",
            "  ✓ FP 3: G1(nodes=15, edges=24) <-> G2(nodes=15, edges=25)\n",
            "  ✓ FP 4: G1(nodes=15, edges=22) <-> G2(nodes=15, edges=24)\n",
            "  ✓ FP 5: G1(nodes=15, edges=23) <-> G2(nodes=15, edges=23)\n",
            "\n",
            "✓ Fingerprints saved to fingerprints/match_fingerprints.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Create Synthetic Fingerprints (Random Graph Pairs)\n",
        "# ============================================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 6: Create Synthetic Fingerprints for Graph Matching\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "num_fingerprints = 5\n",
        "nodes_per_graph = 15\n",
        "\n",
        "def create_random_graph_pair_fingerprint(num_nodes, num_features, sparsity=0.3):\n",
        "    \"\"\"\n",
        "    Create a pair of random graphs as fingerprint.\n",
        "    For graph matching, fingerprints are PAIRS of graphs.\n",
        "    \"\"\"\n",
        "    # ------------------------------\n",
        "    # Graph 1\n",
        "    # ------------------------------\n",
        "    x1 = torch.randn(num_nodes, num_features)\n",
        "    num_edges1 = max(1, int(num_nodes * (num_nodes - 1) / 2 * sparsity))\n",
        "    edge_pairs1 = []\n",
        "    for _ in range(num_edges1):\n",
        "        u = np.random.randint(0, num_nodes)\n",
        "        v = np.random.randint(0, num_nodes)\n",
        "        if u != v:\n",
        "            edge_pairs1.append([u, v])\n",
        "    edge_index1 = torch.tensor(edge_pairs1, dtype=torch.long).t() if edge_pairs1 else torch.zeros((2, 0), dtype=torch.long)\n",
        "    batch1 = torch.zeros(num_nodes, dtype=torch.long)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Graph 2 (slightly different)\n",
        "    # ------------------------------\n",
        "    x2 = torch.randn(num_nodes, num_features)\n",
        "    num_edges2 = max(1, int(num_nodes * (num_nodes - 1) / 2 * sparsity))\n",
        "    edge_pairs2 = []\n",
        "    for _ in range(num_edges2):\n",
        "        u = np.random.randint(0, num_nodes)\n",
        "        v = np.random.randint(0, num_nodes)\n",
        "        if u != v:\n",
        "            edge_pairs2.append([u, v])\n",
        "    edge_index2 = torch.tensor(edge_pairs2, dtype=torch.long).t() if edge_pairs2 else torch.zeros((2, 0), dtype=torch.long)\n",
        "    batch2 = torch.zeros(num_nodes, dtype=torch.long)\n",
        "\n",
        "    return (x1, edge_index1, batch1), (x2, edge_index2, batch2)\n",
        "\n",
        "# ------------------------------\n",
        "# Generate Fingerprints\n",
        "# ------------------------------\n",
        "fingerprints = []\n",
        "print(f\"\\nCreating {num_fingerprints} random graph pair fingerprints...\")\n",
        "\n",
        "for i in range(num_fingerprints):\n",
        "    graph_pair = create_random_graph_pair_fingerprint(\n",
        "        nodes_per_graph,\n",
        "        dataset.num_node_features,  # FIXED for PyG\n",
        "        sparsity=0.25\n",
        "    )\n",
        "    fingerprints.append(graph_pair)\n",
        "    g1, g2 = graph_pair\n",
        "    print(f\"  ✓ FP {i+1}: G1(nodes={g1[0].shape[0]}, edges={g1[1].shape[1]}) \"\n",
        "          f\"<-> G2(nodes={g2[0].shape[0]}, edges={g2[1].shape[1]})\")\n",
        "\n",
        "# ------------------------------\n",
        "# Save Fingerprints\n",
        "# ------------------------------\n",
        "fp_path = base_dir / \"fingerprints\" / \"match_fingerprints.pt\"\n",
        "\n",
        "# ✅ Create directory if missing\n",
        "fp_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "torch.save(fingerprints, fp_path)\n",
        "print(f\"\\n✓ Fingerprints saved to {fp_path}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQk_75xawY9f",
        "outputId": "7783cfb9-ca4b-48ba-80fa-aa7971d2e406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 7: Collect Model Response Vectors (Similarity Scores)\n",
            "======================================================================\n",
            "\n",
            "Collecting similarity responses from TARGET model...\n",
            "Collecting responses from POSITIVE model 0...\n",
            "Collecting responses from POSITIVE model 1...\n",
            "Collecting responses from NEGATIVE model 0...\n",
            "Collecting responses from NEGATIVE model 1...\n",
            "\n",
            "✓ Response vector dimension: 5\n",
            "  (= 5 fingerprint pairs × 1 similarity score each)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Collect Model Response Vectors (Similarity Scores)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 7: Collect Model Response Vectors (Similarity Scores)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def get_matching_response_vector(model, fingerprints):\n",
        "    \"\"\"\n",
        "    Query model on fingerprints and collect similarity scores.\n",
        "    For graph matching, each fingerprint is a pair, producing one similarity score.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    responses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data1, data2 in fingerprints:\n",
        "            # Compute similarity between the two graphs\n",
        "            similarity = model(data1, data2)\n",
        "            responses.append(similarity.unsqueeze(0))\n",
        "\n",
        "    # Concatenate all responses\n",
        "    response_vector = torch.cat(responses)\n",
        "    return response_vector\n",
        "\n",
        "# Collect responses from all models\n",
        "all_responses = {}\n",
        "\n",
        "print(\"\\nCollecting similarity responses from TARGET model...\")\n",
        "all_responses['target'] = get_matching_response_vector(target_model, fingerprints)\n",
        "\n",
        "for i, pos_model in enumerate(positive_models):\n",
        "    print(f\"Collecting responses from POSITIVE model {i}...\")\n",
        "    all_responses[f'pos_{i}'] = get_matching_response_vector(pos_model, fingerprints)\n",
        "\n",
        "for i, neg_model in enumerate(negative_models):\n",
        "    print(f\"Collecting responses from NEGATIVE model {i}...\")\n",
        "    all_responses[f'neg_{i}'] = get_matching_response_vector(neg_model, fingerprints)\n",
        "\n",
        "print(f\"\\n✓ Response vector dimension: {all_responses['target'].shape[0]}\")\n",
        "print(f\"  (= {num_fingerprints} fingerprint pairs × 1 similarity score each)\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlGAVH6Cwdk2",
        "outputId": "b5497d60-cc98-4cf9-82ad-a646367bd1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 8: Build Training Data and Train Verifier\n",
            "======================================================================\n",
            "\n",
            "✓ Target model (label=1)\n",
            "✓ Positive model 0 (label=1)\n",
            "✓ Positive model 1 (label=1)\n",
            "✓ Negative model 0 (label=0)\n",
            "✓ Negative model 1 (label=0)\n",
            "\n",
            "Training data shape: X=torch.Size([5, 5]), y=torch.Size([5])\n",
            "  Class 1 (positive): 3 samples\n",
            "  Class 0 (negative): 2 samples\n",
            "\n",
            "Training VERIFIER for Graph Matching task...\n",
            "  Epoch 50/200 | Loss: 0.0107\n",
            "  Epoch 100/200 | Loss: 0.0012\n",
            "  Epoch 150/200 | Loss: 0.0007\n",
            "  Epoch 200/200 | Loss: 0.0004\n",
            "\n",
            "✓ Verifier saved to verifier/verifier_match.pt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Build Training Data and Train Verifier\n",
        "# ============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 8: Build Training Data and Train Verifier\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Build training dataset for verifier\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "# Positive samples (label = 1)\n",
        "X_train.append(all_responses['target'].unsqueeze(0))\n",
        "y_train.append(1)\n",
        "print(\"\\n✓ Target model (label=1)\")\n",
        "\n",
        "for i in range(len(positive_models)):\n",
        "    X_train.append(all_responses[f'pos_{i}'].unsqueeze(0))\n",
        "    y_train.append(1)\n",
        "    print(f\"✓ Positive model {i} (label=1)\")\n",
        "\n",
        "# Negative samples (label = 0)\n",
        "for i in range(len(negative_models)):\n",
        "    X_train.append(all_responses[f'neg_{i}'].unsqueeze(0))\n",
        "    y_train.append(0)\n",
        "    print(f\"✓ Negative model {i} (label=0)\")\n",
        "\n",
        "X_train = torch.cat(X_train, dim=0)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "print(f\"\\nTraining data shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Class 1 (positive): {(y_train == 1).sum()} samples\")\n",
        "print(f\"  Class 0 (negative): {(y_train == 0).sum()} samples\")\n",
        "\n",
        "# Train verifier\n",
        "print(f\"\\nTraining VERIFIER for Graph Matching task...\")\n",
        "verifier = Verifier(input_dim=X_train.shape[1], hidden_dim=32)\n",
        "optimizer = torch.optim.Adam(verifier.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    verifier.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = verifier(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"  Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save verifier\n",
        "verifier_path = base_dir / \"verifier\" / \"verifier_match.pt\"\n",
        "\n",
        "# ✅ Create the 'verifier' directory if it doesn't exist\n",
        "verifier_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "torch.save(verifier.state_dict(), verifier_path)\n",
        "print(f\"\\n✓ Verifier saved to {verifier_path}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVKMjfMgwtbl",
        "outputId": "0dad8b16-49f3-41b2-fb14-f054c44ff285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 9: EVALUATE VERIFIER - Calculate TP/TN/Accuracy\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CONFUSION MATRIX\n",
            "======================================================================\n",
            "  TP (True Positive):   3   ← Positive models correctly identified\n",
            "  TN (True Negative):   2   ← Negative models correctly identified\n",
            "  FP (False Positive):  0   ← Negative incorrectly as positive\n",
            "  FN (False Negative):  0   ← Positive incorrectly as negative\n",
            "\n",
            "======================================================================\n",
            "METRICS\n",
            "======================================================================\n",
            "  Accuracy:   1.000  (TP+TN)/Total = (3+2)/5\n",
            "  Precision:  1.000  TP/(TP+FP) = 3/(3+0)\n",
            "  Recall:     1.000   TP/(TP+FN) = 3/(3+0)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Evaluate Verifier and Calculate Metrics\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 9: EVALUATE VERIFIER - Calculate TP/TN/Accuracy\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "verifier.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = verifier(X_train)\n",
        "    y_pred = (y_pred_probs >= 0.5).long()\n",
        "    y_true = y_train.long()\n",
        "\n",
        "# Calculate confusion matrix\n",
        "TP = ((y_pred == 1) & (y_true == 1)).sum().item()\n",
        "TN = ((y_pred == 0) & (y_true == 0)).sum().item()\n",
        "FP = ((y_pred == 1) & (y_true == 0)).sum().item()\n",
        "FN = ((y_pred == 0) & (y_true == 1)).sum().item()\n",
        "\n",
        "total = len(y_true)\n",
        "accuracy = (TP + TN) / total\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  TP (True Positive):   {TP}   ← Positive models correctly identified\")\n",
        "print(f\"  TN (True Negative):   {TN}   ← Negative models correctly identified\")\n",
        "print(f\"  FP (False Positive):  {FP}   ← Negative incorrectly as positive\")\n",
        "print(f\"  FN (False Negative):  {FN}   ← Positive incorrectly as negative\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"METRICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Accuracy:   {accuracy:.3f}  (TP+TN)/Total = ({TP}+{TN})/{total}\")\n",
        "print(f\"  Precision:  {precision:.3f}  TP/(TP+FP) = {TP}/({TP}+{FP})\")\n",
        "print(f\"  Recall:     {recall:.3f}   TP/(TP+FN) = {TP}/({TP}+{FN})\")\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
